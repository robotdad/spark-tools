# Spark Tools - Cluster Configuration
# Copy to ~/.config/spark-tools/cluster.env
# Or run: spark-init

# =============================================================================
# Orchestration Mode
# =============================================================================
# swarm  - Docker Swarm + MPI (supports trtllm and vllm engines)
# ray    - Ray cluster (supports vllm engine only)
SPARK_MODE=swarm

# =============================================================================
# Inference Engine
# =============================================================================
# trtllm - TensorRT-LLM (multi-node via Swarm/MPI, high throughput)
# vllm   - vLLM (single-node in swarm mode, multi-node in ray mode)
# NOTE: ray + trtllm is not supported
SPARK_ENGINE=trtllm

# =============================================================================
# Cluster Nodes
# =============================================================================
SPARK_PRIMARY_HOST=monad
SPARK_SECONDARY_HOST=dyad

# =============================================================================
# Network
# =============================================================================
# QSFP interface for inter-node GPU traffic (200Gb/s)
SPARK_QSFP_IFACE=enp1s0f0np0

# API port for inference endpoint
SPARK_PORT=8000

# =============================================================================
# Docker Images
# =============================================================================
SPARK_TRTLLM_IMAGE="nvcr.io/nvidia/tensorrt-llm/release:spark-single-gpu-dev"
SPARK_VLLM_IMAGE="nvcr.io/nvidia/vllm:25.09-py3"
SPARK_RAY_VLLM_IMAGE="scitrera/dgx-spark-vllm:0.15.1-t5"

# =============================================================================
# Auth Proxy (optional)
# =============================================================================
# Set to "true" to enable bearer-token auth proxy in front of API
SPARK_PROXY_ENABLED=false
SPARK_PROXY_PORT=9000
