# Spark Tools - Model Configuration
# Copy to ~/.config/spark-tools/model.env
# Or use: spark-set-model <model-name>

# =============================================================================
# Model Identity
# =============================================================================
MODEL_NAME=nvidia/Qwen3-235B-A22B-FP4
SERVED_MODEL_NAME=

# =============================================================================
# Parallelism & Memory
# =============================================================================
TP_SIZE=2
MAX_MODEL_LEN=32768
GPU_MEM_UTIL=0.85

# =============================================================================
# vLLM Extra Arguments
# =============================================================================
# These apply when SPARK_ENGINE=vllm (both swarm and ray modes)
# --enforce-eager: REQUIRED on GB10 (avoids Triton allocator crash during CUDA graph capture)
# --enable-prefix-caching: KV cache reuse for shared prefixes (system prompts)
# --block-size 128: KV cache block granularity
# --attention-backend flashinfer: optimized attention for this GPU architecture
# --max-num-seqs 16: max concurrent sequences
# --trust-remote-code: required by some HuggingFace models
VLLM_EXTRA_ARGS="--enforce-eager --enable-prefix-caching --block-size 128 --attention-backend flashinfer --max-num-seqs 16 --trust-remote-code"

# =============================================================================
# TRT-LLM Settings
# =============================================================================
# These apply when SPARK_ENGINE=trtllm
TRTLLM_PORT=8355
TRTLLM_MAX_BATCH_SIZE=4
TRTLLM_MAX_NUM_TOKENS=32768
TRTLLM_GPU_MEM_FRACTION=0.9
